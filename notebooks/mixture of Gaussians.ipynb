{
 "metadata": {
  "name": "",
  "signature": "sha256:fb6a1eba9beb144da0d0f265486bec0ffb03d141cd4019e9394f1b3e3891b491"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "#configure plotting\n",
      "%matplotlib inline\n",
      "%config InlineBackend.figure_format = 'svg'\n",
      "import matplotlib\n",
      "matplotlib.rcParams['figure.figsize'] =  (8,5)\n",
      "matplotlib.rcParams['text.usetex'] = True\n",
      "matplotlib.rcParams['font.size'] = 16\n",
      "matplotlib.rcParams['font.family'] = 'serif'\n",
      "\n",
      "import sys\n",
      "sys.path.append('/home/james/work/gpclust/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Mixture of Gaussians with GPclust\n",
      "\n",
      "###James Hensman, November 2014\n",
      "\n",
      "This is a very simple demo to show how to use GPclust to build a mixture of Gaussians. We'll grab some toy data, fit the model and do some very simple analyis of the posterior. We'll also have a look at the truncated Dirichlet process approximation, and how to run a merge-split method to find the optimal number of clusters. Finally we'll examine the effect of changing the priors over the cluster components. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.load('twoD_clustering_example.numpy')\n",
      "print X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(506, 2)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have a two-dimensional dataset, with 506 data.\n",
      "\n",
      "Building the model is as simple as importing the GPclust library and calling the class constructor with our data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import GPclust\n",
      "m = GPclust.MOG(X, Nclust)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'Nclust' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-fc89b17aea0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPclust\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPclust\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMOG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNclust\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'Nclust' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For two-dimensional data, there's a handy built-in plot function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, the data are colored according to the (most probably) assigned cluster, and the heavy line(s) show the contours of the models probability density. Thin lines show the countours of the probablity density of each component.\n",
      "\n",
      "We can see that the model has not been fitted yet! The variational approximation to the posterior is initialized randomly, i.e. all data are randomly assigned across the clusters. To fit the approximation, call the optimize routine, and plot again. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.optimize()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Dirichlet process priors and the merge-split method\n",
      "Instead of using a fixed number of clusters, we can place a Dirichlet process prior over the component weights: effectively a prior of an infintie number of clusters. The variational approximation is _truncated_ se we still only model a finite number of clusters, but now we can optimize for the number of components.\n",
      "\n",
      "To help explore the search space, GPclust has a `try_split` function. This re-initialized the posterior with a cluster 'split' into two. Attempted splits are only accepted if the bound onthe marginal likelihood increases.  A wrapper around the `try_split()`, method, `systematic_splits()` repeatedly attempts splits. \n",
      "\n",
      "Let's build a MOG with a DP prior, truncated at four components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = GPclust.MOG(X, K=4, prior_Z='DP', alpha=10.)\n",
      "m.optimize()\n",
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With only four components, the fit is quite poor. To attempt to split the $i^\\textrm{th}$ component, call `try_split(i)`. The method returns `True` if the split was sucessful, and displays the current status after each split attempt. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.try_split(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To save having to repeatedly call the try_split method, there's a helpful `systematic_splits()` function, which iterates through each cluster and attempts to split it until the marginal-likelihood bound fails to increase. Here we'll turn the output off, as it can get a bit long."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.systematic_splits(verbose=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Getting at the parameters of the posterior. \n",
      "There are perhaps three main tasks that we'd like to do with the fitted model:\n",
      "* See which data are assigned to the same cluster\n",
      "* Examine the means (and variances) of the cluster components\n",
      "* Predict the probability density at a new point\n",
      "\n",
      "The posterior over data assignments is stored in a matix called `phi`. This matrix is NxK, and each element $\\phi_{nk}$ represents the probability that the $n^\\textrm{th}$ datum is assigned to the $k^\\textrm{th}$ component. Here we'll visualize the posterior assignment probabilities in a heatmap-style plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phi = m.phi\n",
      "_=plt.matshow(phi.T, cmap=plt.cm.hot, vmin=0, vmax=1, aspect='auto')\n",
      "plt.xlabel('data index')\n",
      "plt.ylabel('cluster index')\n",
      "_=plt.colorbar()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variational posterior for each of the components in the MOG model is a Gaussian-Wishart distribution. These are stored in the model as so:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'first component mean:', m.mun[:,0]\n",
      "print 'first component covariance:\\n', m.Sns[:,:,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, to obtain the predictive density of the model at any point, we can use `m.predict`. To get the density under each of the components, we can use `m.predict_components'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_point = np.array([[0.1, 0.0]])\n",
      "density = m.predict(test_point)\n",
      "print 'model density:',density\n",
      "\n",
      "cw_density = m.predict_components(test_point)\n",
      "print 'density under each component:', cw_density.round(3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Adjusting the prior parameters\n",
      "The MOG model has priors over the component means and covariances (Gaussian Wishart) and a Dirichlet (or Dirichlet-process) prior over the mixing proportions. We'll use the 2D dataset to illustrate how to change these prior parameters and the effect of them. \n",
      "\n",
      "First, let's place a strong prior over the cluster covariances that forces them to be small, and a large concentration parameter, which makes the Dirichlet process prefer a large number of clusters. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = GPclust.MOG(X, K=Nclust, prior_S=np.eye(2)*5e-3, prior_v=100., prior_Z='DP', alpha=100)\n",
      "m.optimize()\n",
      "m.systematic_splits(verbose=False)\n",
      "m.systematic_splits(verbose=False)\n",
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the model finds a very large number of small clusters! Now let's see what happens if we relax the concentration parameter, and reduce the strength of the Wishart prior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = GPclust.MOG(X, K=Nclust, prior_S=np.eye(2)*5e-3, prior_v=10., prior_Z='DP', alpha=0.1)\n",
      "m.optimize()\n",
      "m.systematic_splits(verbose=False)\n",
      "m.systematic_splits(verbose=False)\n",
      "m.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The MOG also takes a prior mean parameter (which defalts to the mean of the data) and a prior concentration parameter as part of the Wishart distribution. We leave experimenting with these to the reader. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}